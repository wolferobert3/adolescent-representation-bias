{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOvZP9Vx0Vk2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "\n",
        "from os import path, mkdir\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBCuhFRCzZv7"
      },
      "outputs": [],
      "source": [
        "def sc_weat(x, A, B):\n",
        "  \"\"\"\n",
        "  Calculate the SC-WEAT effect size for a single word.\n",
        "  \"\"\"\n",
        "\n",
        "  A_normed = A / A.norm(p=2, dim=1, keepdim=True)\n",
        "  B_normed = B / B.norm(p=2, dim=1, keepdim=True)\n",
        "  x_normed = x / x.norm(p=2, keepdim=True)\n",
        "\n",
        "  A_sims = torch.matmul(A_normed, x_normed)\n",
        "  B_sims = torch.matmul(B_normed, x_normed)\n",
        "  all_sims = torch.cat((A_sims, B_sims), dim=1)\n",
        "\n",
        "  A_mean = A_sims.mean()\n",
        "  B_mean = B_sims.mean()\n",
        "  joint_std = torch.std(all_sims)\n",
        "\n",
        "  effect_size = (A_mean - B_mean) / joint_std\n",
        "\n",
        "  return effect_size\n",
        "\n",
        "def get_similarities(x, A, B):\n",
        "  \"\"\"\n",
        "  Return the raw cosine similarities for a word with two attribute groups.\n",
        "  \"\"\"\n",
        "\n",
        "  A_normed = A / A.norm(p=2, dim=1, keepdim=True)\n",
        "  B_normed = B / B.norm(p=2, dim=1, keepdim=True)\n",
        "  x_normed = x / x.norm(p=2, keepdim=True)\n",
        "\n",
        "  A_sims = torch.matmul(A_normed, x_normed)\n",
        "  B_sims = torch.matmul(B_normed, x_normed)\n",
        "\n",
        "  return A_sims, B_sims\n",
        "\n",
        "def full_vocab_sc_weat(embedding, A, B):\n",
        "  \"\"\"\n",
        "  Calculate the SC-WEAT effect size for each word in the embedding vocabulary.\n",
        "  \"\"\"\n",
        "\n",
        "  A_normed = A / A.norm(p=2, dim=1, keepdim=True)\n",
        "  B_normed = B / B.norm(p=2, dim=1, keepdim=True)\n",
        "  embedding_normed = embedding / embedding.norm(p=2, dim=1, keepdim=True)\n",
        "\n",
        "  assert A_normed.shape[1] == B_normed.shape[1] == embedding_normed.shape[1]\n",
        "\n",
        "  A_sims = torch.matmul(embedding_normed, A_normed.T)\n",
        "  B_sims = torch.matmul(embedding_normed, B_normed.T)\n",
        "  all_sims = torch.cat((A_sims, B_sims), dim=1)\n",
        "\n",
        "  A_means = A_sims.mean(dim=1)\n",
        "  B_means = B_sims.mean(dim=1)\n",
        "  all_stds = all_sims.std(dim=1)\n",
        "\n",
        "  effect_sizes = (A_means - B_means) / all_stds\n",
        "\n",
        "  return effect_sizes\n",
        "\n",
        "def single_term_association(embedding, x):\n",
        "    \"\"\"\n",
        "    Calculate cosine similarity of a single word with each word in the embedding vocabulary.\n",
        "    \"\"\"\n",
        "\n",
        "    x_normed = x / x.norm(p=2)\n",
        "    embedding_normed = embedding / embedding.norm(p=2, dim=1, keepdim=True)\n",
        "\n",
        "    sims = torch.matmul(embedding_normed, x_normed)\n",
        "\n",
        "    return sims\n",
        "\n",
        "def full_vocab_associations(embedding, A):\n",
        "  \"\"\"\n",
        "  Calculate the mean and standard deviation of the cosine similarities between each word in the embedding vocabulary and each word in A.\n",
        "  \"\"\"\n",
        "\n",
        "  A_normed = A / A.norm(p=2, dim=1, keepdim=True)\n",
        "  embedding_normed = embedding / embedding.norm(p=2, dim=1, keepdim=True)\n",
        "\n",
        "  assert A_normed.shape[1] == embedding_normed.shape[1]\n",
        "\n",
        "  A_sims = torch.matmul(embedding_normed, A_normed.T)\n",
        "  A_means = A_sims.mean(dim=1)\n",
        "  A_stds = A_sims.std(dim=1)\n",
        "\n",
        "  return A_means, A_stds\n",
        "\n",
        "# Function to compute p-value of EAT\n",
        "def compute_p_value(associations1: torch.tensor,\n",
        "                    associations2: torch.tensor,\n",
        "                    permutations:int=1000\n",
        "                    ) -> float:\n",
        "    \"\"\"\n",
        "    Computes the one-tailed p-value of an EAT for the given sets of associations with two attribute groups.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute test statistic, difference between sums of associations\n",
        "    test_statistic = associations1.sum() - associations2.sum()\n",
        "\n",
        "    # Create joint distribution of associations by concatenating\n",
        "    joint_sims = torch.cat([associations1, associations2])\n",
        "\n",
        "    # Permute joint distribution to form tensor of shape (permutations, len(joint_sims))\n",
        "    joint_permutations = torch.stack([joint_sims[torch.randperm(len(joint_sims))] for _ in range(permutations)])\n",
        "\n",
        "    # Compute differential associations for each permutation, choosing first len(associations1) for A and last len(associations2) for B\n",
        "    differential_associations = joint_permutations[:, :len(associations1)].sum(dim=1) - joint_permutations[:, len(associations1):].sum(dim=1)\n",
        "\n",
        "    # Compute mean and standard deviation of distribution of permutations\n",
        "    dist_mean, dist_std = differential_associations.mean(), differential_associations.std(correction=1)\n",
        "\n",
        "    # Compute p-value as probability of observing a test statistic as extreme as the one observed, given the null hypothesis\n",
        "    p_value = min(norm.cdf(test_statistic, loc=dist_mean, scale=dist_std), 1 - norm.cdf(test_statistic, loc=dist_mean, scale=dist_std))\n",
        "\n",
        "    return p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPvEV-MnZ-Xi"
      },
      "outputs": [],
      "source": [
        "# Define word stimuli in English and Nepali\n",
        "teenager_words = {\n",
        "    'en': 'teenager',\n",
        "    'ne': 'युवा'\n",
        "}\n",
        "\n",
        "ages_english = {\n",
        "    'teenagers': ['teenager', 'teenagers', 'teen', 'teens', 'teenage', 'teenaged', 'adolescent', 'adolescence'],\n",
        "    'adults' : ['adult', 'adults', 'adulthood', 'middle-age', 'middle-aged', 'grownup', 'grown-up', 'grownups'],\n",
        "    'elderly': ['elder', 'elders', 'elderly', 'aged', 'aging', 'older', 'old-age', 'retiree'],\n",
        "    'children': ['child', 'children', 'childlike', 'childhood', 'kid', 'kids',  'schoolchild', 'schoolchildren']\n",
        "}\n",
        "\n",
        "ages_nepali = {\n",
        "    'teenagers': ['किशोर', 'किशोरी', 'किशोरावस्था', 'कन्या', 'जवान', 'तरुण', 'ल्यासे', 'किशोरहरु'],\n",
        "    'adults': ['वयस्क', 'वयस्कहरू', 'अधबैंसे', 'पौढ', 'परिपक्व', 'परिपक्व', 'हुर्केको', 'बडा'],\n",
        "    'elderly': ['बुढा', 'बुढाहरू', 'बुढ्यौली', 'वृद्ध', 'ज्येष्ठ', 'जेठो', 'बुढेसकाल', 'अभिभावक'],\n",
        "    'children': ['बालक', 'बालकहरू', 'बालिका', 'बालबालिका', 'बच्चा', 'पाठी', 'छात्रा', 'बाल्यकाल']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-gG7OJ-0Uub"
      },
      "outputs": [],
      "source": [
        "def compute_and_write_embedding_results(embedding, embedding_name, write_dir, language='en'):\n",
        "    \"\"\"\n",
        "    Calculate associations and effect sizes for a given embedding and write results to text files.\n",
        "    \"\"\"\n",
        "\n",
        "    # Match word stimuli to language\n",
        "    teenager_word = teenager_words[language]\n",
        "\n",
        "    age_dict = ages_nepali if language.lower()=='ne' else ages_english\n",
        "    teenagers_, adults_, elderly_, children_ = age_dict['teenagers'], age_dict['adults'], age_dict['elderly'], age_dict['children']\n",
        "\n",
        "    # Calculate the cosine similarity of the word \"teenager\" with each word in the embedding vocabulary\n",
        "\n",
        "    teenager_vec = embedding.get_vecs_by_tokens(teenager_word)\n",
        "    teenager_associations = single_term_association(embedding.vectors, teenager_vec)\n",
        "    top_associations = torch.topk(teenager_associations, 1000)\n",
        "\n",
        "    # Create a text file of the 1000 most similar words to the word \"teenager\" and their cosine similarities\n",
        "\n",
        "    write_str = ''\n",
        "    associated_words = []\n",
        "\n",
        "    for idx in range(1000):\n",
        "        word = embedding.itos[top_associations[1][idx]]\n",
        "        similarity = top_associations[0][idx]\n",
        "        write_str += f'{word},{similarity.item()}\\n'\n",
        "        associated_words.append(word)\n",
        "\n",
        "    with open(path.join(write_dir, f'{embedding_name}_teenager_sims.csv'), 'w') as f:\n",
        "        f.write(write_str)\n",
        "\n",
        "    # Check that all words are in the embedding vocabulary\n",
        "\n",
        "    missing_words = []\n",
        "\n",
        "    for word in teenagers_ + adults_ + elderly_ + children_:\n",
        "        if word not in embedding.itos:\n",
        "            missing_words.append(word)\n",
        "\n",
        "    if missing_words:\n",
        "        with open(path.join(write_dir, f'{embedding_name}_missing_words.txt'), 'w') as f:\n",
        "            f.write('\\n'.join(missing_words))\n",
        "\n",
        "    # Load the embedding vectors for each set of word stimuli\n",
        "\n",
        "    teenager_vecs = embedding.get_vecs_by_tokens(teenagers_)\n",
        "    adult_vecs = embedding.get_vecs_by_tokens(adults_)\n",
        "    elderly_vecs = embedding.get_vecs_by_tokens(elderly_)\n",
        "    child_vecs = embedding.get_vecs_by_tokens(children_)\n",
        "\n",
        "    # Get mean and standard deviation of cosine similarities between each word in the embedding vocabulary and each word in the teenager set\n",
        "    teenager_means, teenager_stds = full_vocab_associations(embedding.vectors, teenager_vecs)\n",
        "    top_associations = torch.topk(teenager_means, 1000)\n",
        "\n",
        "    # Create a dictionary of the top 1000 words and their mean cosine similarities and standard deviations\n",
        "    teenager_mean_dict = {}\n",
        "\n",
        "    for topk_idx in range(1000):\n",
        "        similarity = top_associations[0][topk_idx]\n",
        "        mean_index = top_associations[1][topk_idx]\n",
        "        std = teenager_stds[mean_index]\n",
        "        word = embedding.itos[mean_index]\n",
        "        teenager_mean_dict[word] = (similarity, std)\n",
        "\n",
        "    # Write the dictionary to a text file\n",
        "\n",
        "    write_str = ''\n",
        "\n",
        "    for word in teenager_mean_dict:\n",
        "        similarity, std = teenager_mean_dict[word]\n",
        "        write_str += f'{word},{similarity.item()},{std.item()}\\n'\n",
        "\n",
        "    with open(path.join(write_dir, f'{embedding_name}_teenager_means.csv'), 'w') as f:\n",
        "        f.write(write_str)\n",
        "\n",
        "    # Calculate the SC-WEAT effect size for each word in the embedding vocabulary for each set of word stimuli\n",
        "\n",
        "    teen_adult_associations = full_vocab_sc_weat(embedding.vectors, teenager_vecs, adult_vecs)\n",
        "    teen_elderly_associations = full_vocab_sc_weat(embedding.vectors, teenager_vecs, elderly_vecs)\n",
        "    teen_child_associations = full_vocab_sc_weat(embedding.vectors, teenager_vecs, child_vecs)\n",
        "\n",
        "    # Concatenate the effect sizes for each set of word stimuli into a single tensor\n",
        "    intersected_associations = torch.cat((teen_adult_associations.unsqueeze(0).T, teen_elderly_associations.unsqueeze(0).T, teen_child_associations.unsqueeze(0).T), dim=1)\n",
        "\n",
        "    # Take only large effect sizes (effect size >= 0.8) demonstrating association with the teenager set for each set of word stimuli\n",
        "    large_associations = torch.all((intersected_associations >= 0.8), dim=1)\n",
        "\n",
        "    # Create a dictionary of the 1000 most frequent words and their effect sizes for each set of word stimuli\n",
        "\n",
        "    association_dict = {}\n",
        "    count = 0\n",
        "\n",
        "    while len(association_dict) < 1000 and count < len(embedding.itos): # Note that words are ordered by frequency\n",
        "\n",
        "        if large_associations[count]:\n",
        "\n",
        "            # Get the target embedding\n",
        "            target_embedding = embedding[count]\n",
        "\n",
        "            # Compute p-values for WEATs - done only for large associations to prevent wasting resources\n",
        "            p1 = compute_p_value(*get_similarities(target_embedding, teenager_vecs, adult_vecs))\n",
        "            p2 = compute_p_value(*get_similarities(target_embedding, teenager_vecs, elderly_vecs))\n",
        "            p3 = compute_p_value(*get_similarities(target_embedding, teenager_vecs, child_vecs))\n",
        "\n",
        "            # Exclude any word + similarity that is not statistically significant\n",
        "            if p1 > .05 or p2 > .05 or p3 > .05:\n",
        "              continue\n",
        "\n",
        "            # Add significant words to the dictionary\n",
        "            word = embedding.itos[count]\n",
        "            association_dict[word] = (intersected_associations[count][0], intersected_associations[count][1], intersected_associations[count][2])\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Write the dictionary to a text file\n",
        "\n",
        "    write_str = ''\n",
        "\n",
        "    for word in association_dict:\n",
        "        adult, elderly, child = association_dict[word]\n",
        "        write_str += f'{word},{adult.item()},{elderly.item()},{child.item()}\\n'\n",
        "\n",
        "    with open(path.join(write_dir, f'{embedding_name}_teenager_effect_sizes.csv'), 'w') as f:\n",
        "        f.write(write_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e70G36o7UDj"
      },
      "outputs": [],
      "source": [
        "# Define the path to the embedding files\n",
        "WRITE_DIR = f'./swe_results/'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "if not path.exists(WRITE_DIR):\n",
        "    mkdir(WRITE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectors = torchtext.vocab.Vectors(name='glove.840B.300d.txt', cache='./.vector_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute and write the results for the GloVe 840B embedding\n",
        "compute_and_write_embedding_results(vectors, 'glove_840B', WRITE_DIR, language='en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectors = torchtext.vocab.Vectors(name='crawl-300d-2M.vec', cache='./.vector_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute and write the results for the FT 300D 2M embedding\n",
        "compute_and_write_embedding_results(vectors, 'ft_2m', WRITE_DIR, language='en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectors = torchtext.vocab.Vectors(name='nepali_glove_vectors.txt', cache='./.vector_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compute_and_write_embedding_results(vectors, 'glove_ne', WRITE_DIR, language='ne')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectors = torchtext.vocab.Vectors(name='cc.ne.300.vec', cache='./.vector_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compute_and_write_embedding_results(vectors, 'ft_ne_cc', WRITE_DIR, language='ne')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
