{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_vad_words(df: pd.DataFrame,\n",
    "                      language: str,\n",
    "                      result_type: str,\n",
    "                      write_dir: str = 'human_results_clustering') -> None:\n",
    "    \"\"\"\n",
    "    Cluster words based on VAD scores and write to file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get target words and embeddings\n",
    "    words = df.index.values\n",
    "    embeddings = np.stack([df.loc[word].values for word in words])\n",
    "\n",
    "    # Examine best number of clusters\n",
    "    max_, max_idx = 0, 0\n",
    "\n",
    "    # Iterate through number of clusters\n",
    "    for i in range(5, 11):\n",
    "\n",
    "        kmeans = KMeans(n_clusters=i, random_state=0).fit(embeddings)\n",
    "        score = silhouette_score(embeddings, kmeans.labels_, metric='cosine')\n",
    "        \n",
    "        if score > max_:\n",
    "            max_ = score\n",
    "            max_idx = i\n",
    "\n",
    "    # Fit kmeans with best number of clusters\n",
    "    kmeans = KMeans(n_clusters=max_idx, random_state=0, n_init=\"auto\").fit(embeddings)\n",
    "\n",
    "    # Create string to write to file\n",
    "    write_string = f'{language}_{result_type}: {max_idx} clusters'\n",
    "\n",
    "    # Iterate through clusters and collect sentences in each cluster\n",
    "    for i in range(max_idx):\n",
    "        write_string += f'\\nCluster {i}: {len([j for j in kmeans.labels_ if j == i])/len(kmeans.labels_)*100:.2f}%'\n",
    "        for j in range(len(kmeans.labels_)):\n",
    "            if kmeans.labels_[j] == i:\n",
    "                write_string += f'\\n\\t{j+1}: {words[j]}'\n",
    "\n",
    "    # Write to file\n",
    "    with open(f'{write_dir}/{language}_{result_type}_clusters.txt', 'w') as f:\n",
    "        f.write(write_string)\n",
    "\n",
    "    # UMAP for dimensionality reduction\n",
    "    reducer = umap.UMAP(n_components=2, metric='cosine', random_state=0)\n",
    "    umap_embedding = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Create tsv with word, dim1, dim2, cluster\n",
    "    tsv_string = 'word\\tdim1\\tdim2\\tcluster\\n'\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        tsv_string += f'{words[i]}\\t{umap_embedding[i][0]}\\t{umap_embedding[i][1]}\\t{kmeans.labels_[i]}\\n'\n",
    "\n",
    "    with open(f'{write_dir}/{language}_{result_type}_clusters.tsv', 'w') as f:\n",
    "        f.write(tsv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_wordlist(human_df: pd.DataFrame,\n",
    "                        exclude_words: list,\n",
    "                        include_words: list) -> list:\n",
    "    \"\"\"\n",
    "    Preprocess wordlist to remove non-words and duplicates.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = human_df.columns\n",
    "\n",
    "    words = [human_df[col].values for col in columns]\n",
    "    words = [word for sublist in words for word in sublist]\n",
    "    words = [word.lower().strip().replace(' ','') for word in words if type(word) == str]\n",
    "    words = [word for word in words if '/' not in word and word not in exclude_words]\n",
    "    words = words + include_words\n",
    "    words = list(set(words))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in VAD word norms - see https://saifmohammad.com/WebPages/nrc-vad.html\n",
    "word_norms = pd.read_csv('./NRC-VAD-Lexicon.txt', sep='\\t', header=None, names=['Word', 'Valence', 'Arousal', 'Dominance'])\n",
    "word_norms = word_norms.set_index('Word')\n",
    "word_norms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = [\n",
    "    'english_humans_most_associated.csv',\n",
    "    'english_humans_uniquely_associated.csv',\n",
    "    'nepali_humans_most_associated.csv',\n",
    "    'nepali_humans_uniquely_associated.csv',\n",
    "]\n",
    "\n",
    "exclude_lists = [\n",
    "    ['maluable','excersise','influentiable', 'judgemental'],\n",
    "    [\"party's\",'college-admissons','self-concious','adolscence'],\n",
    "    [],\n",
    "    []\n",
    "]\n",
    "\n",
    "include_lists = [\n",
    "    ['family', 'parents', 'malleable', 'exercise', 'influenceable', 'judgmental'],\n",
    "    ['parties','college-admissions','self-conscious','adolescence'],\n",
    "    [],\n",
    "    []\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_sources)):\n",
    "\n",
    "    data_source = data_sources[i]\n",
    "    language = data_source.split('_')[0]\n",
    "    result_type = data_source.split('_')[2]\n",
    "\n",
    "    human_data_df = pd.read_csv(f'./human_data/{data_sources[i]}')\n",
    "\n",
    "    words = preprocess_wordlist(human_data_df, exclude_lists[i], include_lists[i])\n",
    "    words = [word for word in words if word in word_norms.index]\n",
    "    word_features = word_norms.loc[words]\n",
    "\n",
    "    cluster_vad_words(word_features, language, result_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
